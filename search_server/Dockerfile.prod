# Stage 1: 모델을 미리 다운로드하여 캐시하는 빌더 이미지
FROM python:3.9.20-slim AS model-builder

RUN pip install --no-cache-dir huggingface_hub[cli]==0.24.6

# BAAI/bge-reranker-v2-m3 모델의 필요한 파일 다운로드 및 캐시 저장
RUN huggingface-cli download BAAI/bge-reranker-v2-m3 config.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 sentencepiece.bpe.model --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 tokenizer.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 model.safetensors --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 special_tokens_map.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-reranker-v2-m3 tokenizer_config.json --cache-dir=/model_cache

# BAAI/bge-m3 모델의 전체 파일 다운로드 및 캐시 저장
RUN huggingface-cli download BAAI/bge-m3 config_sentence_transformers.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 modules.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 README.md --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 sentence_bert_config.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 config.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 pytorch_model.bin --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 tokenizer_config.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 sentencepiece.bpe.model --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 tokenizer.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 special_tokens_map.json --cache-dir=/model_cache && \
    huggingface-cli download BAAI/bge-m3 1_Pooling/config.json --cache-dir=/model_cache


# Stage 2: 최종 런타임 이미지
FROM python:3.9.20-slim

WORKDIR /app

# Stage 1에서 모델 캐시 파일을 복사
COPY --from=model-builder /model_cache /root/.cache/huggingface/hub

COPY requirements.txt .

RUN pip install --no-cache-dir  pip==24.2 &&  pip install --no-cache-dir -r requirements.txt

COPY ./src ./src

WORKDIR /app/src

EXPOSE 8000

ENV ENVIRONMENT=prod
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
ENV CUDA_VISIBLE_DEVICES=0
ARG SENTRY_DSN
ENV SENTRY_DSN=$SENTRY_DSN
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
